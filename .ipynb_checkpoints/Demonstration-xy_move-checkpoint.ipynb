{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "INITIAL_EPSILON = 0.5 # starting value of epsilon\n",
    "FINAL_EPSILON = 0.01 # final value of epsilon\n",
    "REPLAY_SIZE = 10000 # experience replay buffer size\n",
    "BATCH_SIZE = 32 # size of minibatch\n",
    "GAMMA = 0.9 # discount factor for target Q\n",
    "\n",
    "class agent():\n",
    "    def __init__(self, env):\n",
    "        # self.state_dim = env.observation_space.shape[0]\n",
    "        self.state_dim = 4\n",
    "        # self.action_dim = env.action_space.n\n",
    "        self.action_dim = 4\n",
    "        self.create_Q_network()  # otherwise: AttributeError: agent instance has no attribute 'Q_value'\n",
    "\n",
    "        self.create_training_method()\n",
    "        self.teacher_create()\n",
    "\n",
    "        # Otherwise: ValueError: Cannot evaluate tensor using `eval()`: No default session is registered. \n",
    "        # Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`\n",
    "        # Init session\n",
    "        self.session = tf.InteractiveSession()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Otherwise: AttributeError: agent instance has no attribute 'epsilon'\n",
    "        self.epsilon = INITIAL_EPSILON\n",
    "\n",
    "        # init experience replay\n",
    "        self.replay_buffer = deque()\n",
    "\n",
    "        # init some parameters\n",
    "        self.time_step = 0\n",
    "\n",
    "        # The position of this code should not be there, but before \"self.session\"\n",
    "        # self.create_training_method()\n",
    "    def create_Q_network(self):\n",
    "        # input: state, \n",
    "        # output: value of action\n",
    "\n",
    "        # network weights\n",
    "        W1 = self.weight_variable([self.state_dim,20])\n",
    "        b1 = self.bias_variable([20])\n",
    "        W2 = self.weight_variable([20,self.action_dim])\n",
    "        b2 = self.bias_variable([self.action_dim])\n",
    "        # input layer\n",
    "        self.state_input = tf.placeholder(\"float\",[None,self.state_dim])\n",
    "        # hidden layers\n",
    "        h_layer = tf.nn.relu(tf.matmul(self.state_input,W1) + b1)\n",
    "        # Q Value layer\n",
    "        self.Q_value = tf.matmul(h_layer,W2) + b2\n",
    "    def weight_variable(self,shape):\n",
    "        initial = tf.truncated_normal(shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(self,shape):\n",
    "        initial = tf.constant(0.01, shape = shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def egreedy_action(self,state):\n",
    "        Q_value = self.Q_value.eval(feed_dict = {\n",
    "          self.state_input:[state]\n",
    "          })[0]\n",
    "        # the value of  \"INITIAL_EPSILON\" and \"FINAL_EPSILON\" will not change\n",
    "        # but the value of \"self.epsilon\" will decrease\n",
    "        self.epsilon -= (INITIAL_EPSILON - FINAL_EPSILON)/10000\n",
    "\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randint(0,self.action_dim - 1)\n",
    "        else:\n",
    "            return np.argmax(Q_value)\n",
    "    def sess_close(self):\n",
    "        self.session.close()\n",
    "    \n",
    "    def perceive(self,state,action,reward,next_state,done):\n",
    "        one_hot_action = np.zeros(self.action_dim)\n",
    "        one_hot_action[action] = 1\n",
    "        self.replay_buffer.append((state,one_hot_action,reward,next_state,done))\n",
    "        # print self.replay_buffer\n",
    "        if len(self.replay_buffer) > REPLAY_SIZE-3000:\n",
    "            self.replay_buffer.popleft()\n",
    "        # train very frequently\n",
    "        if len(self.replay_buffer) > BATCH_SIZE:\n",
    "            self.train_Q_network()\n",
    "    def teacher_create(self):\n",
    "\n",
    "        A=deque()\n",
    "        target_x=0\n",
    "        target_y=0\n",
    "        move_x=20\n",
    "        move_y=20\n",
    "        for i in range(20):\n",
    "            state=np.array([target_x, target_y, move_x, move_y])\n",
    "            move_x=move_x-1\n",
    "            next_state=np.array([target_x, target_y, move_x, move_y])\n",
    "            A.append(( state, np.array([ 0.,  0.,  0.,  1.]), -1, next_state, 0 ))\n",
    "\n",
    "        for i in range(19):\n",
    "            state=np.array([target_x, target_y, move_x, move_y])\n",
    "            move_y=move_y-1\n",
    "            next_state=np.array([target_x, target_y, move_x, move_y])\n",
    "            A.append(( state, np.array([ 0.,  1.,  0.,  0.]), -1, next_state, 0 ))\n",
    "\n",
    "        if 1==1:\n",
    "            state=np.array([target_x, target_y, move_x, move_y])\n",
    "            move_y=move_y-1\n",
    "            next_state=np.array([target_x, target_y, move_x, move_y])\n",
    "            A.append(( state, np.array([ 0.,  1.,  0.,  0.]), 20, next_state, 1 ))\n",
    "\n",
    "        while len(A)<3000:\n",
    "            A+=A\n",
    "        while len(A)>3000:\n",
    "            A.popleft()\n",
    "\n",
    "        self.teacher_deque=A\n",
    "    def train_Q_network(self):\n",
    "        # Step 1: obtain random minibatch from replay memory\n",
    "        self.time_step += 1\n",
    "        tmp_deque=deque()\n",
    "        tmp_deque+=self.teacher_deque\n",
    "        tmp_deque+=self.replay_buffer\n",
    "\n",
    "        minibatch = random.sample(self.replay_buffer,BATCH_SIZE)\n",
    "        state_batch = [data[0] for data in minibatch]\n",
    "        action_batch = [data[1] for data in minibatch]\n",
    "        reward_batch = [data[2] for data in minibatch]\n",
    "        next_state_batch = [data[3] for data in minibatch]\n",
    "\n",
    "        Q_value_batch = self.Q_value.eval(feed_dict={self.state_input:next_state_batch})\n",
    "\n",
    "        # Step 2: calculate y\n",
    "        y_batch = []\n",
    "        for i in range(0,BATCH_SIZE):\n",
    "            done = minibatch[i][4]\n",
    "            if done:\n",
    "                y_batch.append(reward_batch[i]) # always be 1\n",
    "            else :\n",
    "                y_batch.append(reward_batch[i] + GAMMA * np.max(Q_value_batch[i]))\n",
    "\n",
    "        self.optimizer.run(feed_dict={self.y_input: y_batch, self.action_input: action_batch, self.state_input: state_batch})\n",
    "\n",
    "    def create_training_method(self):\n",
    "        self.action_input = tf.placeholder(\"float\",[None,self.action_dim]) # one hot presentation\n",
    "        self.y_input = tf.placeholder(\"float\",[None])\n",
    "        # Notice: Q_value is decided by \"self.state_input:\"\n",
    "        Q_action = tf.reduce_sum(tf.multiply(self.Q_value,self.action_input),reduction_indices = 1)\n",
    "        self.cost = tf.reduce_mean(tf.square(self.y_input - Q_action))\n",
    "        self.optimizer = tf.train.AdamOptimizer(0.0001).minimize(self.cost)\n",
    "    def action(self,state):\n",
    "        return np.argmax(self.Q_value.eval(feed_dict = {self.state_input:[state]})[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --> total_reward:  -300\n",
      "1 --> total_reward:  -300\n",
      "done! with test:  0\n",
      "2 --> total_reward:  -42\n",
      "3 --> total_reward:  -300\n",
      "done! with test:  0\n",
      "4 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "5 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "6 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "7 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "8 --> total_reward:  -40\n",
      "9 --> total_reward:  -300\n",
      "10 --> total_reward:  -300\n",
      "11 --> total_reward:  -300\n",
      "done! with test:  0\n",
      "12 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "13 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "14 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "15 --> total_reward:  -40\n",
      "16 --> total_reward:  -300\n",
      "17 --> total_reward:  -300\n",
      "18 --> total_reward:  -300\n",
      "19 --> total_reward:  -300\n",
      "20 --> total_reward:  -300\n",
      "done! with test:  0\n",
      "21 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "22 --> total_reward:  -40\n",
      "23 --> total_reward:  -300\n",
      "24 --> total_reward:  -300\n",
      "done! with test:  0\n",
      "25 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "26 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "27 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "28 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "29 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "30 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "31 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "32 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "33 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "34 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "35 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "36 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "37 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "38 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "39 --> total_reward:  -44\n",
      "done! with test:  0\n",
      "40 --> total_reward:  -40\n",
      "41 --> total_reward:  -300\n",
      "42 --> total_reward:  -300\n",
      "43 --> total_reward:  -300\n",
      "done! with test:  0\n",
      "44 --> total_reward:  -40\n",
      "45 --> total_reward:  -300\n",
      "done! with test:  0\n",
      "46 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "47 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "48 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "49 --> total_reward:  -40\n",
      "50 --> total_reward:  -300\n",
      "51 --> total_reward:  -300\n",
      "done! with test:  0\n",
      "52 --> total_reward:  -42\n",
      "53 --> total_reward:  -300\n",
      "done! with test:  0\n",
      "54 --> total_reward:  -42\n",
      "55 --> total_reward:  -300\n",
      "done! with test:  0\n",
      "56 --> total_reward:  -42\n",
      "done! with test:  0\n",
      "57 --> total_reward:  -42\n",
      "58 --> total_reward:  -300\n",
      "59 --> total_reward:  -300\n",
      "60 --> total_reward:  -300\n",
      "61 --> total_reward:  -300\n",
      "done! with test:  0\n",
      "62 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "63 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "64 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "65 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "66 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "67 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "68 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "69 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "70 --> total_reward:  -40\n",
      "71 --> total_reward:  -300\n",
      "done! with test:  0\n",
      "72 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "73 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "74 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "75 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "76 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "77 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "78 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "79 --> total_reward:  -40\n",
      "80 --> total_reward:  -300\n",
      "done! with test:  0\n",
      "81 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "82 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "83 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "84 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "85 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "86 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "87 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "88 --> total_reward:  -40\n",
      "89 --> total_reward:  -300\n",
      "done! with test:  0\n",
      "90 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "91 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "92 --> total_reward:  -40\n",
      "93 --> total_reward:  -300\n",
      "done! with test:  0\n",
      "94 --> total_reward:  -40\n",
      "95 --> total_reward:  -300\n",
      "96 --> total_reward:  -300\n",
      "done! with test:  0\n",
      "97 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "98 --> total_reward:  -40\n",
      "done! with test:  0\n",
      "99 --> total_reward:  -40\n"
     ]
    }
   ],
   "source": [
    "TRAIN_EPISODE = 100\n",
    "EPISODE = 100 # Episode limitation\n",
    "\n",
    "import class_xy_move\n",
    "\n",
    "# step 1: connect the gym\n",
    "# import gym\n",
    "env = class_xy_move.xy_move()\n",
    "\n",
    "my_agent = agent(env)\n",
    "\n",
    "reward_save = []\n",
    "\n",
    "for i_test_episode in range(TRAIN_EPISODE):\n",
    "    k=0\n",
    "    for episode in range(EPISODE):\n",
    "        state = env.reset()\n",
    "        \n",
    "        # should avoid infinite loop here:\n",
    "        for __ in range(300):\n",
    "            # env.render() # maybe work for the display of car-pole\n",
    "            # action = env.action_space.sample()\n",
    "            action = my_agent.egreedy_action(state)\n",
    "            next_state,reward,done,_ = env.step(action)\n",
    "            my_agent.perceive(state,action,reward,next_state,done)\n",
    "            state = next_state\n",
    "            # print state,action,reward,next_state,done\n",
    "            if done:\n",
    "                # print \"done! with run: \", k\n",
    "                # k=k+1\n",
    "                break\n",
    "\n",
    "    # my_agent.train_Q_network()\n",
    "\n",
    "    # wether the network be better, test it\n",
    "    total_reward = 0\n",
    "    for i in range(1):\n",
    "        state = env.reset()\n",
    "        \n",
    "        # should avoid infinite loop here:\n",
    "        for __ in range(300):\n",
    "            # env.render()\n",
    "            action = my_agent.action(state) # direct action for test\n",
    "            state,reward,done,_ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                print( \"done! with test: \", i)\n",
    "                break\n",
    "    print( i_test_episode, \"--> total_reward: \", total_reward)\n",
    "    reward_save.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2111bca3da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xypoints = reward_save\n",
    "\n",
    "plt.plot(np.array(xypoints).reshape(-1), 'g*')#, label='--'\n",
    "plt.title('Title')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "# plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 眼睛里要容得下沙子"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
